{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for the 2 Object Spring Experiments\n",
    "This notebook can be used to load and analyze the results generated by running `training_2ObjectSpring.py`. The notebook is meant to evaluate multiple experiments together and compute the average Performances over all experiments. \n",
    "\n",
    "The path to the the folder containing the experiments needs to be specified under `path_experiment`. The folder set here needs to contain the subfolders with the experiments, where those subfolders need to contain `ckpt.pth` and `./hydra/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from torchvision import utils\n",
    "from models.sceneRepresentation import Scene\n",
    "from dataset.dataset import ImageDataset_paig\n",
    "from util.util import compute_psnr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the experiments to analyze.\n",
    "# The folder specified here needs to contain the subfolders which contain `ckpt.pth` and `./hydra/` \n",
    "path_experiments = os.path.join(\n",
    "    os.path.abspath(''),\n",
    "    'experiments',\n",
    "    '2023-01-24',\n",
    "    'Spring'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_errors = []\n",
    "psnrs = []\n",
    "\n",
    "# Get all the experiments\n",
    "for path_experiment in os.scandir(path_experiments):\n",
    "    # Load Config\n",
    "    path_conf = os.path.join(path_experiment, '.hydra','config.yaml')\n",
    "    with open(path_conf) as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    print(\"Doing batch_idx: \", cfg['data']['batch_idx'])\n",
    "\n",
    "    # Load Model\n",
    "    model = Scene(**cfg['scene']['background'])\n",
    "\n",
    "    model.add_2ObjectsSpring(\n",
    "        **cfg['scene']['local_representation']\n",
    "    )\n",
    "\n",
    "    path_ckpt = os.path.join(path_experiment, 'ckpt.pth')\n",
    "    model.load_state_dict(torch.load(path_ckpt))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Load Data\n",
    "    data_path = os.path.join(os.path.abspath(''), 'data',cfg['data']['path_data'])\n",
    "\n",
    "    data = ImageDataset_paig(\n",
    "            path_data=data_path,\n",
    "            batch_idx=cfg['data']['batch_idx'],\n",
    "            use_subsampling=False,\n",
    "        )\n",
    "    H, W = data.get_image_dim()\n",
    "\n",
    "    # Compute Parameter Errors\n",
    "    true_k = 2.0\n",
    "    true_l = 12.0\n",
    "    rel_error_k = torch.abs(true_k - model.local_representation.ode.k.data) / true_k\n",
    "    rel_error_l = torch.abs(true_l - W*model.local_representation.ode.eq_distance.data) / true_l\n",
    "\n",
    "    # Compute PSNR and IoU\n",
    "    tspan = data.t_steps.to(device)\n",
    "    model.update_trafo(tspan)\n",
    "    output = model.render_image(W, H)\n",
    "    psnr = compute_psnr(output['Image'].cpu(), data.get_full_images())\n",
    "\n",
    "    param_errors.append(rel_error_k)\n",
    "    param_errors.append(rel_error_l)\n",
    "    psnrs.append(psnr)\n",
    "\n",
    "    mean_errors = torch.mean(torch.tensor([rel_error_k, rel_error_l]))\n",
    "    print(f\"Mean Errors: {mean_errors}, PSNR: {psnr}\")\n",
    "\n",
    "    print(\"Done\")\n",
    "    print(\"====================================================\")\n",
    "\n",
    "avg_param_error = torch.mean(torch.tensor(param_errors))\n",
    "median_param_error = torch.median(torch.tensor(param_errors))\n",
    "avg_psnr = torch.mean(torch.tensor(psnrs))\n",
    "print(\"Results:\")\n",
    "print(f\"Avg Param Error: {avg_param_error}, Median Param Error: {median_param_error} Avg PSNR: {avg_psnr}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Images\n",
    "The following code creates images for a specific experiment. The folder containing `ckpt.pth` and `./hydra/` for this experiment needs to be specified under `path_experiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_experiment = os.path.join(\n",
    "    os.path.abspath(''),\n",
    "    'experiments',\n",
    "    '2023-01-24',\n",
    "    'Spring',\n",
    "    '09-45-53_seq0'\n",
    ")\n",
    "\n",
    "path_conf = os.path.join(path_experiment, '.hydra','config.yaml')\n",
    "with open(path_conf) as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = Scene(**cfg['scene']['background'])\n",
    "\n",
    "model.add_2ObjectsSpring(\n",
    "    **cfg['scene']['local_representation']\n",
    ")\n",
    "\n",
    "path_ckpt = os.path.join(path_experiment, 'ckpt.pth')\n",
    "model.load_state_dict(torch.load(path_ckpt))\n",
    "\n",
    "model.to(device)\n",
    "print(\"Moved to device\")\n",
    "\n",
    "data_path = os.path.join(os.path.abspath(''), 'data',cfg['data']['path_data'])\n",
    "\n",
    "render_dataset = ImageDataset_paig(\n",
    "        path_data=data_path,\n",
    "        batch_idx=cfg['data']['batch_idx'],\n",
    "    )\n",
    "\n",
    "tspan_render = render_dataset.t_steps.to(device)\n",
    "\n",
    "model.eval()\n",
    "model.update_trafo(tspan_render)\n",
    "H, W = render_dataset.get_image_dim()\n",
    "output = model.render_image(W, H)\n",
    "ims = output[\"Image\"].cpu()\n",
    "\n",
    "plt.imshow(ims[0].cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(ims[25].cpu().numpy())\n",
    "\n",
    "path_folder_rendering = os.path.join(path_experiment, 'renderings')\n",
    "os.makedirs(path_folder_rendering)\n",
    "\n",
    "# Save individual images\n",
    "for i in range(len(tspan_render)):\n",
    "    path = os.path.join(path_folder_rendering, f\"{i:02}_ours.jpg\")\n",
    "    cur_im = ims[i].permute(2, 0, 1)\n",
    "    utils.save_image(cur_im, path)\n",
    "\n",
    "    path = os.path.join(path_folder_rendering, f\"{i:02}_gt.jpg\")\n",
    "    cur_im_gt = render_dataset.get_full_images(i).permute(2, 0, 1)\n",
    "\n",
    "\n",
    "print(\"PSNR: \", compute_psnr(ims, render_dataset.get_full_images()))\n",
    "print(\"Error k: \", torch.abs(2.0 - model.local_representation.ode.k.data) / 2.0)\n",
    "print(\"Error l: \", torch.abs(12.0 - W*model.local_representation.ode.eq_distance.data) / 12.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render segmentation masks\n",
    "Creates a plot of the segmentation masks used for the segmentation loss on the first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = render_dataset.get_full_images(0)\n",
    "masks = render_dataset.get_full_mask(0)\n",
    "\n",
    "blend = torch.tensor([0., 0.5, 0.]).unsqueeze(0).unsqueeze(0).repeat(ims.shape[0], ims.shape[1], 1)\n",
    "mask = masks['masks1'].float().unsqueeze(-1) * 0.6\n",
    "blended = mask*blend + (1-mask)*ims\n",
    "plt.imshow(blended)\n",
    "plt.show()\n",
    "\n",
    "path = os.path.join(path_folder_rendering, f\"init1.jpg\")\n",
    "utils.save_image(blended.permute(2, 0, 1), path)\n",
    "\n",
    "blend = torch.tensor([0.5, 0., 0.]).unsqueeze(0).unsqueeze(0).repeat(ims.shape[0], ims.shape[1], 1)\n",
    "mask = masks['masks2'].float().unsqueeze(-1) * 0.6\n",
    "blended = mask*blend + (1-mask)*ims\n",
    "plt.imshow(blended)\n",
    "path = os.path.join(path_folder_rendering, f\"init2.jpg\")\n",
    "utils.save_image(blended.permute(2, 0, 1), path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f01482bfefa3a3bc7460951111586304597956327628e7e66e099af897f7956"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nerf-pytorch3d': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
